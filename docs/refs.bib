@ARTICLE{shannonMathematicalTheoryCommunication1948,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal},
  title={A mathematical theory of communication},
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}}

# usage of Entropy
@article{acharya2024representative,
  title = {How Representative Are Air Transport Functional Complex Networks? {{A}} Quantitative Validation},
  author = {Acharya, Kishor and Olivares, Felipe and Zanin, Massimiliano},
  year = {2024},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {34},
  number = {4},
  publisher = {AIP Publishing}
}

@article{Hartley1928TransmissionOI,
  title = {Transmission of Information},
  author = {Hartley, R. V. L.},
  year = {1928},
  journal = {Bell System Technical Journal},
  volume = {7},
  pages = {535--563}
}

@incollection{jizbaInformationTheoryGeneralized2004,
  title = {Information {{Theory}} and {{Generalized Statistics}}},
  booktitle = {Decoherence and {{Entropy}} in {{Complex Systems}}: {{Selected Lectures}} from {{DICE}} 2002},
  author = {Jizba, Petr},
  editor = {Elze, Hans-Thomas},
  year = {2004},
  pages = {362--376},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-40968-7_26},
  urldate = {2024-12-04},
  abstract = {In this lecture we present a discussion of generalized statistics based on R{\'e}nyi's, Fisher's and Tsallis's measures of information. The unifying conceptual framework which we employ here is provided by information theory. Important applications of generalized statistics to systems with (multi-)fractal structure are examined.},
  isbn = {978-3-540-40968-7},
  langid = {english}
}

@book{khinchin1957mathematical,
  title = {Mathematical Foundations of Information Theory},
  author = {Khinchin, A.I.},
  year = {1957},
  publisher = {Dover},
  address = {New York}
}

@book{kolmogoroff1933,
  title = {Grundbegriffe Der Wahrscheinlichkeitsrechnung},
  author = {Kolmogoroff, A. N.},
  year = {1933},
  publisher = {Berlin}
}


@book{renyi1970probability,
  author    = {A. R\'enyi},
  title     = {Probability Theory},
  publisher = {North-Holland},
  address   = {Amsterdam},
  year      = {1970}
}

@book{renyi1976selected,
  author    = {A. R\'enyi},
  title     = {Selected Papers of Alfred R\'enyi, Vol. 2},
  publisher = {Akad\'emia Kiado},
  address   = {Budapest},
  year      = {1976}
}

@book{cover2012elements,
  title={Elements of Information Theory},
  author={Cover, T.M. and Thomas, J.A.},
  isbn={9781118585771},
  lccn={2005047799},
  url={https://books.google.ee/books?id=VWq5GG6ycxMC},
  year={2012},
  publisher={Wiley}
}

@book{manning1999foundations,
  title={Foundations of Statistical Natural Language Processing},
  author={Manning, C. and Schutze, H.},
  isbn={9780262133609},
  lccn={99021137},
  series={Foundations of Statistical Natural Language Processing},
  url={https://books.google.ee/books?id=YiFDxbEX3SUC},
  year={1999},
  publisher={MIT Press}
}

#Symbolic Entropy
@article{PermutationEntropy2002,
  title = {Permutation Entropy: A Natural Complexity Measure for Time Series},
  author = {Bandt, Christoph and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {88},
  issue = {17},
  pages = {174102},
  numpages = {4},
  year = {2002},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.88.174102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102}
}



# local Entropy
@Inbook{Lizier2014,
author="Lizier, Joseph T.",
editor="Wibral, Michael
and Vicente, Raul
and Lizier, Joseph T.",
title="Measuring the Dynamics of Information Processing on a Local Scale in Time and Space",
bookTitle="Directed Information Measures in Neuroscience",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="161--193",
abstract="Studies of how information is processed in natural systems, in particular in nervous systems, are rapidly gaining attention. Less known however is that the local dynamics of such information processing in space and time can be measured. In this chapter, we review the mathematics of how to measure local entropy and mutual information values at specific observations of time-series processes.We then review how these techniques are used to construct measures of local information storage and transfer within a distributed system, and we describe how these measures can reveal much more intricate details about the dynamics of complex systems than their more well-known ``average'' measures do. This is done by examining their application to cellular automata, a classic complex system, where these local information profiles have provided quantitative evidence for long-held conjectures regarding the information transfer and processing role of gliders and glider collisions. Finally, we describe the outlook in anticipating the broad application of these local measures of information processing in computational neuroscience.",
isbn="978-3-642-54474-3",
doi="10.1007/978-3-642-54474-3_7",
url="https://doi.org/10.1007/978-3-642-54474-3_7"
}

#local MI
@book{fano1961transmission,
  author    = {Fano, R. M.},
  title     = {Transmission of Information: A Statistical Theory of Communications},
  publisher = {M.I.T. Press},
  address   = {Cambridge, MA, USA},
  year      = {1961},
  note      = {See Chapter 2}
}



@article{Jizba2003,
author = {Jizba, Peter},
year = {2003},
month = {02},
pages = {},
title = {Information Theory and Generalized Statistics},
volume = {633},
isbn = {978-3-540-20639-2},
doi = {10.1007/978-3-540-40968-7_26}
}
@article{articleTsallis,
author = {Tsallis, Constantino},
year = {1988},
month = {07},
pages = {479-487},
title = {Possible generalization of Boltzmann-Gibbs statistics},
volume = {52},
journal = {Journal of Statistical Physics},
doi = {10.1007/BF01016429}
}

@article{Tsallis1999,
  author    = {C. Tsallis},
  title     = {Nonextensive statistics: theoretical, experimental and computational evidences and connections},
  journal   = {Braz. J. Phys.},
  volume    = {29},
  year      = {1999},
  pages     = {1},
}

@article{Tsallis1998,
  author    = {C. Tsallis and R.S. Mandes and A.R. Plastino},
  title     = {The role of constraints within generalized nonextensive statistics},
  journal   = {Physica A},
  volume    = {261},
  year      = {1998},
  pages     = {534},
}

@misc{TsallisBibliography,
  author    = {C. Tsallis},
  title     = {Tsallis Bibliography},
  note      = {\url{http://tsallis.cat.cbpf.br/biblio.htm}},
}

@article{RevieEstimators,
author = {Hlavackova-Schindler, Katerina and Palus, Milan and Vejmelka, Martin and Bhattacharya, Joydeep},
year = {2007},
month = {02},
pages = {},
title = {Causality Detection Based on Information-Theoretic Approaches in Time Series Analysis},
volume = {441 (2007) 1 â€“ 46},
journal = {Physics Reports}
}

% kernel Entropy Estimator
@book{silverman1986density,
  title={Density Estimation for Statistics and Data Analysis},
  author={Silverman, B.W.},
  year={1986},
  publisher={Chapman and Hall},
  address={London},
  url={http://dx.doi.org/10.1007/978-1-4899-3324-9}
}
% KL Entropy estimator
@article{kozachenko1987sample,
  title = {Sample Estimate of the Entropy of a Random Vector},
  author = {Kozachenko, L.F. and Leonenko, N.N.},
  year = {1987},
  journal = {Problemy Peredachi Informatsii},
  volume = {23},
  pages = {95--100}
}

@article{laisantNumerationFactorielleApplication1888,
  title = {Sur La Num{\'e}ration Factorielle, Application Aux Permutations},
  author = {Laisant, C.- A.},
  year = {1888},
  journal = {Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
  volume = {2},
  pages = {176--183},
  issn = {0037-9484, 2102-622X},
  doi = {10.24033/bsmf.378},
  urldate = {2024-10-10}
}

@inproceedings{Lehmer1960TeachingCT,
  title = {Teaching Combinatorial Tricks to a Computer},
  booktitle = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
  author = {Lehmer, D. H.},
  year = {1960},
  series = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
  volume = {10},
  publisher = {American Mathematical Society},
  address = {Providence, Rhode Island},
  doi = {10.1090/psapm/010}
}

# Renyi Entropy
@article{leonenkoClassRenyiInformation2008,
  title = {A Class of {{R{\'e}nyi}} Information Estimators for Multidimensional Densities},
  author = {Leonenko, Nikolai and Pronzato, Luc and Savani, Vippal},
  year = {2008},
  journal = {The Annals of Statistics},
  volume = {36},
  number = {5},
  pages = {2153--2182},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/07-AOS539},
  keywords = {Entropy estimation,estimation of divergence,estimation of statistical distance,Havrda-Charvat entropy,nearest-neighbor distances,Renyi entropy,Tsallis entropy}
}

@inproceedings{leonenkoEstimationEntropiesDivergences2006,
  title = {Estimation of Entropies and Divergences via Nearest Neighbors},
  booktitle = {{{ProbaStat}} 2006},
  author = {Leonenko, Nikolai and Pronzato, Luc and Savani, Vippal},
  year = {2006},
  month = jun,
  volume = {39},
  pages = {265--273},
  address = {Smolenice, Slovakia},
  hal_id = {hal-00322783},
  hal_version = {v1},
  keywords = {entropy estimation,estimation of divergence,estimation of statistical distance,Havrda-Chav\`at entropy,nearest-neighbor distances,R\'enyi entropy,Tsallis entropy}
}

@article{miKSG2004,
  title = {Erratum: {{Estimating}} Mutual Information [Phys. {{Rev}}. {{E}} 69, 066138 (2004)]},
  author = {Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  year = {2011},
  month = jan,
  journal = {Physical Review E},
  volume = {83},
  doi = {10.1103/PhysRevE.83.019903}
}


% TE
@article{Schreiber.paper,
  title = {Measuring Information Transfer},
  author = {Schreiber, Thomas},
  journal = {Phys. Rev. Lett.},
  volume = {85},
  issue = {2},
  pages = {461--464},
  numpages = {0},
  year = {2000},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.85.461},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.85.461}
}

@article{article_KSG_TE,
author = {Gomez-Herrero, German and Wu, Wei and Rutanen, Kalle and Soriano, Miguel and Pipa, Gordon and Vicente, Raul},
year = {2010},
month = {08},
pages = {},
title = {Assessing Coupling Dynamics from an Ensemble of Time Series},
volume = {17},
journal = {Entropy},
doi = {10.3390/e17041958}
}

@article{article_eTE_Computation,
author = {Marschinski, R. and Kantz, H.},
year = {2002},
month = {11},
pages = {275-281},
title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
volume = {30},
journal = {European Physical Journal B},
doi = {10.1140/epjb/e2002-00379-2}
}

@article{articleKantz,
author = {Marschinski, R. and Kantz, H.},
year = {2002},
month = {11},
pages = {275-281},
title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
volume = {30},
journal = {European Physical Journal B},
doi = {10.1140/epjb/e2002-00379-2}
}

@article{TE_Kernel_Kaiser,
author = {Kaiser, A. and Schreiber, Thomas},
year = {2002},
month = {06},
pages = {},
title = {Information transfer in continuous processes},
volume = {166},
journal = {Physica D, v.166, 43-62 (2002)},
doi = {10.1016/S0167-2789(02)00432-3}
}

@article{KSG_TE,
    doi = {10.1371/journal.pone.0055809},
    author = {Wibral, Michael AND Pampu, Nicolae AND Priesemann, Viola AND SiebenhÃ¼hner, Felix AND Seiwert, Hannes AND Lindner, Michael AND Lizier, Joseph T. AND Vicente, Raul},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Measuring Information-Transfer Delays},
    year = {2013},
    month = {02},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pone.0055809},
    pages = {1-19},
    abstract = {In complex networks such as gene networks, traffic systems or brain circuits it is important to understand how long it takes for the different parts of the network to effectively influence one another. In the brain, for example, axonal delays between brain areas can amount to several tens of milliseconds, adding an intrinsic component to any timing-based processing of information. Inferring neural interaction delays is thus needed to interpret the information transfer revealed by any analysis of directed interactions across brain structures. However, a robust estimation of interaction delays from neural activity faces several challenges if modeling assumptions on interaction mechanisms are wrong or cannot be made. Here, we propose a robust estimator for neuronal interaction delays rooted in an information-theoretic framework, which allows a model-free exploration of interactions. In particular, we extend transfer entropy to account for delayed source-target interactions, while crucially retaining the conditioning on the embedded target state at the immediately previous time step. We prove that this particular extension is indeed guaranteed to identify interaction delays between two coupled systems and is the only relevant option in keeping with Wienerâ€™s principle of causality. We demonstrate the performance of our approach in detecting interaction delays on finite data by numerical simulations of stochastic and deterministic processes, as well as on local field potential recordings. We also show the ability of the extended transfer entropy to detect the presence of multiple delays, as well as feedback loops. While evaluated on neuroscience data, we expect the estimator to be useful in other fields dealing with network dynamics.},
    number = {2},

}

@article{Symbolic_TE,
  title = {Symbolic Transfer Entropy},
  author = {Staniek, Matth\"aus and Lehnertz, Klaus},
  journal = {Phys. Rev. Lett.},
  volume = {100},
  issue = {15},
  pages = {158101},
  numpages = {4},
  year = {2008},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.100.158101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.158101}
}

@Inbook{Lizier2014_localinfomeasure,
author="Lizier, Joseph T.",
editor="Wibral, Michael
and Vicente, Raul
and Lizier, Joseph T.",
title="Measuring the Dynamics of Information Processing on a Local Scale in Time and Space",
bookTitle="Directed Information Measures in Neuroscience",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="161--193",
abstract="Studies of how information is processed in natural systems, in particular in nervous systems, are rapidly gaining attention. Less known however is that the local dynamics of such information processing in space and time can be measured. In this chapter, we review the mathematics of how to measure local entropy and mutual information values at specific observations of time-series processes.We then review how these techniques are used to construct measures of local information storage and transfer within a distributed system, and we describe how these measures can reveal much more intricate details about the dynamics of complex systems than their more well-known ``average'' measures do. This is done by examining their application to cellular automata, a classic complex system, where these local information profiles have provided quantitative evidence for long-held conjectures regarding the information transfer and processing role of gliders and glider collisions. Finally, we describe the outlook in anticipating the broad application of these local measures of information processing in computational neuroscience.",
isbn="978-3-642-54474-3",
doi="10.1007/978-3-642-54474-3_7",
url="https://doi.org/10.1007/978-3-642-54474-3_7"
}

#JSD
@ARTICLE{Divergence_measures_Lin,
  author={Lin, J.},
  journal={IEEE Transactions on Information Theory},
  title={Divergence measures based on the Shannon entropy},
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  keywords={Entropy;Probability distribution;Upper bound;Pattern analysis;Signal analysis;Signal processing;Pattern recognition;Taxonomy;Genetics;Computer science},
  doi={10.1109/18.61115}}

# Conditional MI (KSG)
@article{CMI_KSG_Frenzel_Pompe,
  title = {Partial Mutual Information for Coupling Analysis of Multivariate Time Series},
  author = {Frenzel, Stefan and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {99},
  issue = {20},
  pages = {204101},
  numpages = {4},
  year = {2007},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.99.204101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.204101}
}

#local TE
@article{local_TE_Lizier,
  title = {Local information transfer as a spatiotemporal filter for complex systems},
  author = {Lizier, Joseph T. and Prokopenko, Mikhail and Zomaya, Albert Y.},
  journal = {Phys. Rev. E},
  volume = {77},
  issue = {2},
  pages = {026110},
  numpages = {11},
  year = {2008},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.77.026110},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.77.026110}
}

#conditional TE article
@Article{cond_TE_Ostargard,
AUTHOR = {Shahsavari Baboukani, Payam and Graversen, Carina and Alickovic, Emina and Ã˜stergaard, Jan},
TITLE = {Estimating Conditional Transfer Entropy in Time Series Using Mutual Information and Nonlinear Prediction},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1124},
URL = {https://www.mdpi.com/1099-4300/22/10/1124},
PubMedID = {33286893},
ISSN = {1099-4300},
ABSTRACT = {We propose a new estimator to measure directed dependencies in time series. The dimensionality of data is first reduced using a new non-uniform embedding technique, where the variables are ranked according to a weighted sum of the amount of new information and improvement of the prediction accuracy provided by the variables. Then, using a greedy approach, the most informative subsets are selected in an iterative way. The algorithm terminates, when the highest ranked variable is not able to significantly improve the accuracy of the prediction as compared to that obtained using the existing selected subsets. In a simulation study, we compare our estimator to existing state-of-the-art methods at different data lengths and directed dependencies strengths. It is demonstrated that the proposed estimator has a significantly higher accuracy than that of existing methods, especially for the difficult case, where the data are highly correlated and coupled. Moreover, we show its false detection of directed dependencies due to instantaneous couplings effect is lower than that of existing measures. We also show applicability of the proposed estimator on real intracranial electroencephalography data.},
DOI = {10.3390/e22101124}
}

@ARTICLE{JSD_distance_Endres,
  author={Endres, D.M. and Schindelin, J.E.},
  journal={IEEE Transactions on Information Theory},
  title={A new metric for probability distributions},
  year={2003},
  volume={49},
  number={7},
  pages={1858-1860},
  keywords={Gaussian noise;Probability distribution;Iterative algorithms;Writing;Algorithm design and analysis;Wavelet analysis;Adaptive estimation;White noise;Bayesian methods;Convergence},
  doi={10.1109/TIT.2003.813506}}
