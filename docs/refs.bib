
@article{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C. E.},
  year = {1948},
  month = oct,
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {4},
  pages = {623--656},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1948.tb00917.x},
  urldate = {2024-06-25},
  langid = {english}
}

@article{Hartley1928TransmissionOI,
  title={Transmission of information},
  author={R. V. L. Hartley},
  journal={Bell System Technical Journal},
  year={1928},
  volume={7},
  pages={535-563},
  url={https://api.semanticscholar.org/CorpusID:109872947}
}
@book{khinchin1957mathematical,
  author    = {A.I. Khinchin},
  title     = {Mathematical Foundations of Information Theory},
  publisher = {Dover},
  address   = {New York},
  year      = {1957}
}

@book{kolmogoroff1933,
  author    = {A. N. Kolmogoroff},
  title     = {Grundbegriffe der Wahrscheinlichkeitsrechnung},
  year      = {1933},
  publisher = {Berlin}
}


@book{renyi1970probability,
  author    = {A. R\'enyi},
  title     = {Probability Theory},
  publisher = {North-Holland},
  address   = {Amsterdam},
  year      = {1970}
}

@book{renyi1976selected,
  author    = {A. R\'enyi},
  title     = {Selected Papers of Alfred R\'enyi, Vol. 2},
  publisher = {Akad\'emia Kiado},
  address   = {Budapest},
  year      = {1976}
}

@book{cover2012elements,
  title={Elements of Information Theory},
  author={Cover, T.M. and Thomas, J.A.},
  isbn={9781118585771},
  lccn={2005047799},
  url={https://books.google.ee/books?id=VWq5GG6ycxMC},
  year={2012},
  publisher={Wiley}
}

@book{manning1999foundations,
  title={Foundations of Statistical Natural Language Processing},
  author={Manning, C. and Schutze, H.},
  isbn={9780262133609},
  lccn={99021137},
  series={Foundations of Statistical Natural Language Processing},
  url={https://books.google.ee/books?id=YiFDxbEX3SUC},
  year={1999},
  publisher={MIT Press}
}

# local Entropy
@Inbook{Lizier2014,
author="Lizier, Joseph T.",
editor="Wibral, Michael
and Vicente, Raul
and Lizier, Joseph T.",
title="Measuring the Dynamics of Information Processing on a Local Scale in Time and Space",
bookTitle="Directed Information Measures in Neuroscience",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="161--193",
abstract="Studies of how information is processed in natural systems, in particular in nervous systems, are rapidly gaining attention. Less known however is that the local dynamics of such information processing in space and time can be measured. In this chapter, we review the mathematics of how to measure local entropy and mutual information values at specific observations of time-series processes.We then review how these techniques are used to construct measures of local information storage and transfer within a distributed system, and we describe how these measures can reveal much more intricate details about the dynamics of complex systems than their more well-known ``average'' measures do. This is done by examining their application to cellular automata, a classic complex system, where these local information profiles have provided quantitative evidence for long-held conjectures regarding the information transfer and processing role of gliders and glider collisions. Finally, we describe the outlook in anticipating the broad application of these local measures of information processing in computational neuroscience.",
isbn="978-3-642-54474-3",
doi="10.1007/978-3-642-54474-3_7",
url="https://doi.org/10.1007/978-3-642-54474-3_7"
}

#local MI
@book{fano1961transmission,
  author    = {Fano, R. M.},
  title     = {Transmission of Information: A Statistical Theory of Communications},
  publisher = {M.I.T. Press},
  address   = {Cambridge, MA, USA},
  year      = {1961},
  note      = {See Chapter 2}
}



@article{Jizba2003,
author = {Jizba, Peter},
year = {2003},
month = {02},
pages = {},
title = {Information Theory and Generalized Statistics},
volume = {633},
isbn = {978-3-540-20639-2},
doi = {10.1007/978-3-540-40968-7_26}
}
@article{articleTsallis,
author = {Tsallis, Constantino},
year = {1988},
month = {07},
pages = {479-487},
title = {Possible generalization of Boltzmann-Gibbs statistics},
volume = {52},
journal = {Journal of Statistical Physics},
doi = {10.1007/BF01016429}
}

@article{Tsallis1999,
  author    = {C. Tsallis},
  title     = {Nonextensive statistics: theoretical, experimental and computational evidences and connections},
  journal   = {Braz. J. Phys.},
  volume    = {29},
  year      = {1999},
  pages     = {1},
}

@article{Tsallis1998,
  author    = {C. Tsallis and R.S. Mandes and A.R. Plastino},
  title     = {The role of constraints within generalized nonextensive statistics},
  journal   = {Physica A},
  volume    = {261},
  year      = {1998},
  pages     = {534},
}

@misc{TsallisBibliography,
  author    = {C. Tsallis},
  title     = {Tsallis Bibliography},
  note      = {\url{http://tsallis.cat.cbpf.br/biblio.htm}},
}

@article{RevieEstimators,
author = {Hlavackova-Schindler, Katerina and Palus, Milan and Vejmelka, Martin and Bhattacharya, Joydeep},
year = {2007},
month = {02},
pages = {},
title = {Causality Detection Based on Information-Theoretic Approaches in Time Series Analysis},
volume = {441 (2007) 1 – 46},
journal = {Physics Reports}
}

% kernel Entropy Estimator
@book{silverman1986density,
  title={Density Estimation for Statistics and Data Analysis},
  author={Silverman, B.W.},
  year={1986},
  publisher={Chapman and Hall},
  address={London},
  url={http://dx.doi.org/10.1007/978-1-4899-3324-9}
}
% KL Entropy estimator
@article{kozachenko1987sample,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, L.F. and Leonenko, N.N.},
  journal={Probl. Inform. Transm.},
  volume={23},
  year={1987},
  pages={95--100}
}
% Symbolic Entropy Estimator
@article{PermutationEntropy2002,
  title = {Permutation Entropy: A Natural Complexity Measure for Time Series},
  author = {Bandt, Christoph and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {88},
  issue = {17},
  pages = {174102},
  numpages = {4},
  year = {2002},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.88.174102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102}
}

% Renyi and Tsallis Estimator
@article{RenyiTsallisEstimator2008,
author = {Leonenko, Nikolai and Pronzato, Luc and Savani, Vippal},
year = {2008},
month = {01},
pages = {},
title = {Estimation of entropies and divergences via nearest neighbors}
}
% Renyi Estimator
@article{LeonenkoRenyiEstimator,
author = {Nikolai Leonenko and Luc Pronzato and Vippal Savani},
title = {{A class of Rényi information estimators for multidimensional densities}},
volume = {36},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {2153 -- 2182},
keywords = {Entropy estimation, estimation of divergence, estimation of statistical distance, Havrda–Charvát entropy, nearest-neighbor distances, Rényi entropy, Tsallis entropy},
year = {2008},
doi = {10.1214/07-AOS539},
URL = {https://doi.org/10.1214/07-AOS539}
}


%MI KSG
@article{miKSG2004,
author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
year = {2011},
month = {01},
pages = {},
title = {Erratum: Estimating mutual information [Phys. Rev. E 69, 066138 (2004)]},
volume = {83},
journal = {Physical Review E},
doi = {10.1103/PhysRevE.83.019903}
}


% TE
@article{Schreiber.paper,
  title = {Measuring Information Transfer},
  author = {Schreiber, Thomas},
  journal = {Phys. Rev. Lett.},
  volume = {85},
  issue = {2},
  pages = {461--464},
  numpages = {0},
  year = {2000},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.85.461},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.85.461}
}

@article{article_KSG_TE,
author = {Gomez-Herrero, German and Wu, Wei and Rutanen, Kalle and Soriano, Miguel and Pipa, Gordon and Vicente, Raul},
year = {2010},
month = {08},
pages = {},
title = {Assessing Coupling Dynamics from an Ensemble of Time Series},
volume = {17},
journal = {Entropy},
doi = {10.3390/e17041958}
}

@article{article_eTE_Computation,
author = {Marschinski, R. and Kantz, H.},
year = {2002},
month = {11},
pages = {275-281},
title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
volume = {30},
journal = {European Physical Journal B},
doi = {10.1140/epjb/e2002-00379-2}
}

@article{articleKantz,
author = {Marschinski, R. and Kantz, H.},
year = {2002},
month = {11},
pages = {275-281},
title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
volume = {30},
journal = {European Physical Journal B},
doi = {10.1140/epjb/e2002-00379-2}
}

@article{TE_Kernel_Kaiser,
author = {Kaiser, A. and Schreiber, Thomas},
year = {2002},
month = {06},
pages = {},
title = {Information transfer in continuous processes},
volume = {166},
journal = {Physica D, v.166, 43-62 (2002)},
doi = {10.1016/S0167-2789(02)00432-3}
}

@article{KSG_TE,
    doi = {10.1371/journal.pone.0055809},
    author = {Wibral, Michael AND Pampu, Nicolae AND Priesemann, Viola AND Siebenhühner, Felix AND Seiwert, Hannes AND Lindner, Michael AND Lizier, Joseph T. AND Vicente, Raul},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Measuring Information-Transfer Delays},
    year = {2013},
    month = {02},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pone.0055809},
    pages = {1-19},
    abstract = {In complex networks such as gene networks, traffic systems or brain circuits it is important to understand how long it takes for the different parts of the network to effectively influence one another. In the brain, for example, axonal delays between brain areas can amount to several tens of milliseconds, adding an intrinsic component to any timing-based processing of information. Inferring neural interaction delays is thus needed to interpret the information transfer revealed by any analysis of directed interactions across brain structures. However, a robust estimation of interaction delays from neural activity faces several challenges if modeling assumptions on interaction mechanisms are wrong or cannot be made. Here, we propose a robust estimator for neuronal interaction delays rooted in an information-theoretic framework, which allows a model-free exploration of interactions. In particular, we extend transfer entropy to account for delayed source-target interactions, while crucially retaining the conditioning on the embedded target state at the immediately previous time step. We prove that this particular extension is indeed guaranteed to identify interaction delays between two coupled systems and is the only relevant option in keeping with Wiener’s principle of causality. We demonstrate the performance of our approach in detecting interaction delays on finite data by numerical simulations of stochastic and deterministic processes, as well as on local field potential recordings. We also show the ability of the extended transfer entropy to detect the presence of multiple delays, as well as feedback loops. While evaluated on neuroscience data, we expect the estimator to be useful in other fields dealing with network dynamics.},
    number = {2},

}

@article{Symbolic_TE,
  title = {Symbolic Transfer Entropy},
  author = {Staniek, Matth\"aus and Lehnertz, Klaus},
  journal = {Phys. Rev. Lett.},
  volume = {100},
  issue = {15},
  pages = {158101},
  numpages = {4},
  year = {2008},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.100.158101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.158101}
}

@Inbook{Lizier2014_localinfomeasure,
author="Lizier, Joseph T.",
editor="Wibral, Michael
and Vicente, Raul
and Lizier, Joseph T.",
title="Measuring the Dynamics of Information Processing on a Local Scale in Time and Space",
bookTitle="Directed Information Measures in Neuroscience",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="161--193",
abstract="Studies of how information is processed in natural systems, in particular in nervous systems, are rapidly gaining attention. Less known however is that the local dynamics of such information processing in space and time can be measured. In this chapter, we review the mathematics of how to measure local entropy and mutual information values at specific observations of time-series processes.We then review how these techniques are used to construct measures of local information storage and transfer within a distributed system, and we describe how these measures can reveal much more intricate details about the dynamics of complex systems than their more well-known ``average'' measures do. This is done by examining their application to cellular automata, a classic complex system, where these local information profiles have provided quantitative evidence for long-held conjectures regarding the information transfer and processing role of gliders and glider collisions. Finally, we describe the outlook in anticipating the broad application of these local measures of information processing in computational neuroscience.",
isbn="978-3-642-54474-3",
doi="10.1007/978-3-642-54474-3_7",
url="https://doi.org/10.1007/978-3-642-54474-3_7"
}

#JSD
@ARTICLE{Divergence_measures_Lin,
  author={Lin, J.},
  journal={IEEE Transactions on Information Theory},
  title={Divergence measures based on the Shannon entropy},
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  keywords={Entropy;Probability distribution;Upper bound;Pattern analysis;Signal analysis;Signal processing;Pattern recognition;Taxonomy;Genetics;Computer science},
  doi={10.1109/18.61115}}

  @article{Permutation_Jensen_Shannon_distance,
  title = {Permutation Jensen-Shannon distance: A versatile and fast symbolic tool for complex time-series analysis},
  author = {Zunino, Luciano and Olivares, Felipe and Ribeiro, Haroldo V. and Rosso, Osvaldo A.},
  journal = {Phys. Rev. E},
  volume = {105},
  issue = {4},
  pages = {045310},
  numpages = {21},
  year = {2022},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.105.045310},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.105.045310}
}

# Conditional MI (KSG)
@article{CMI_KSG_Frenzel_Pompe,
  title = {Partial Mutual Information for Coupling Analysis of Multivariate Time Series},
  author = {Frenzel, Stefan and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {99},
  issue = {20},
  pages = {204101},
  numpages = {4},
  year = {2007},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.99.204101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.204101}
}
