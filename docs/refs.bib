@ARTICLE{shannonMathematicalTheoryCommunication1948,
    author = {Shannon, C. E.},
    journal = {The Bell System Technical Journal},
    title = {A mathematical theory of communication},
    year = {1948},
    volume = {27},
    number = {3},
    pages = {379-423},
    keywords = {},
    doi = {10.1002/j.1538-7305.1948.tb01338.x} }

# usage of Entropy

@article{acharya2024representative,
  title = {How Representative Are Air Transport Functional Complex Networks? {{A}} Quantitative Validation},
  shorttitle = {How Representative Are Air Transport Functional Complex Networks?},
  author = {Acharya, Kishor and Olivares, Felipe and Zanin, Massimiliano},
  year = {2024},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {34},
  number = {4},
  pages = {043133},
  issn = {1054-1500},
  doi = {10.1063/5.0189642},
  urldate = {2025-04-16},
  abstract = {Functional networks have emerged as powerful instruments to characterize the propagation of information in complex systems, with applications ranging from neuroscience to climate and air transport. In spite of their success, reliable methods for validating the resulting structures are still missing, forcing the community to resort to expert knowledge or simplified models of the system's dynamics. We here propose the use of a real-world problem, involving the reconstruction of the structure of flights in the US air transport system from the activity of individual airports, as a way to explore the limits of such an approach. While the true connectivity is known and is, therefore, possible to provide a quantitative benchmark, this problem presents challenges commonly found in other fields, including the presence of non-stationarities and observational noise, and the limitedness of available time series. We explore the impact of elements like the specific functional metric employed, the way of detrending the time series, or the size of the reconstructed system and discuss how the conclusions here drawn could have implications for similar analyses in neuroscience.}
}


@article{Hartley1928TransmissionOI,
    title = {Transmission of Information},
    author = {Hartley, R. V. L.},
    year = {1928},
    journal = {Bell System Technical Journal},
    volume = {7},
    pages = {535--563}
}

@incollection{jizbaInformationTheoryGeneralized2004,
    title = {Information {{Theory}} and {{Generalized Statistics}}},
    booktitle = {Decoherence and {{Entropy}} in {{Complex Systems}}: {{Selected Lectures}} from {{DICE}} 2002},
    author = {Jizba, Petr},
    editor = {Elze, Hans-Thomas},
    year = {2004},
    pages = {362--376},
    publisher = {Springer},
    address = {Berlin, Heidelberg},
    doi = {10.1007/978-3-540-40968-7_26},
    urldate = {2024-12-04},
    abstract = {In this lecture we present a discussion of generalized statistics based on R{\'e}nyi's, Fisher's and Tsallis's measures of information. The unifying conceptual framework which we employ here is provided by information theory. Important applications of generalized statistics to systems with (multi-)fractal structure are examined.},
    isbn = {978-3-540-40968-7},
    langid = {english}
}

@book{khinchin1957mathematical,
    title = {Mathematical Foundations of Information Theory},
    author = {Khinchin, A.I.},
    year = {1957},
    publisher = {Dover},
    address = {New York}
}

@book{kolmogoroff1933,
    title = {Grundbegriffe Der Wahrscheinlichkeitsrechnung},
    author = {Kolmogoroff, A. N.},
    year = {1933},
    publisher = {Berlin}
}


@book{renyi1970probability,
    author = {A. R\'enyi},
    title = {Probability Theory},
    publisher = {North-Holland},
    address = {Amsterdam},
    year = {1970}
}

@book{renyi1976selected,
    author = {A. R\'enyi},
    title = {Selected Papers of Alfred R\'enyi, Vol. 2},
    publisher = {Akad\'emia Kiado},
    address = {Budapest},
    year = {1976}
}

@book{cover2012elements,
    title = {Elements of Information Theory},
    author = {Cover, T.M. and Thomas, J.A.},
    isbn = {9781118585771},
    lccn = {2005047799},
    url = {https://books.google.ee/books?id=VWq5GG6ycxMC},
    year = {2012},
    publisher = {Wiley}
}

@book{manning1999foundations,
    title = {Foundations of Statistical Natural Language Processing},
    author = {Manning, C. and Schutze, H.},
    isbn = {9780262133609},
    lccn = {99021137},
    series = {Foundations of Statistical Natural Language Processing},
    url = {https://books.google.ee/books?id=YiFDxbEX3SUC},
    year = {1999},
    publisher = {MIT Press}
}

#Ordinal Entropy

@article{PermutationEntropy2002,
    title = {Permutation Entropy: A Natural Complexity Measure for Time Series},
    author = {Bandt, Christoph and Pompe, Bernd},
    journal = {Phys. Rev. Lett.},
    volume = {88},
    issue = {17},
    pages = {174102},
    numpages = {4},
    year = {2002},
    month = {Apr},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.88.174102},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102}
}



# local Entropy

@Inbook{Lizier2014,
    author = "Lizier, Joseph T.",
    editor = "Wibral, Michael and Vicente, Raul and Lizier, Joseph T.",
    title = "Measuring the Dynamics of Information Processing on a Local Scale in Time and Space",
    bookTitle = "Directed Information Measures in Neuroscience",
    year = "2014",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "161--193",
    abstract = "Studies of how information is processed in natural systems, in particular in nervous systems, are rapidly gaining attention. Less known however is that the local dynamics of such information processing in space and time can be measured. In this chapter, we review the mathematics of how to measure local entropy and mutual information values at specific observations of time-series processes.We then review how these techniques are used to construct measures of local information storage and transfer within a distributed system, and we describe how these measures can reveal much more intricate details about the dynamics of complex systems than their more well-known ``average'' measures do. This is done by examining their application to cellular automata, a classic complex system, where these local information profiles have provided quantitative evidence for long-held conjectures regarding the information transfer and processing role of gliders and glider collisions. Finally, we describe the outlook in anticipating the broad application of these local measures of information processing in computational neuroscience.",
    isbn = "978-3-642-54474-3",
    doi = "10.1007/978-3-642-54474-3_7",
    url = "https://doi.org/10.1007/978-3-642-54474-3_7"
}

#local MI

@book{fano1961transmission,
    author = {Fano, R. M.},
    title = {Transmission of Information: A Statistical Theory of Communications},
    publisher = {M.I.T. Press},
    address = {Cambridge, MA, USA},
    year = {1961},
    note = {See Chapter 2}
}

@article{articleTsallis,
    author = {Tsallis, Constantino},
    year = {1988},
    month = {07},
    pages = {479-487},
    title = {Possible generalization of Boltzmann-Gibbs statistics},
    volume = {52},
    journal = {Journal of Statistical Physics},
    doi = {10.1007/BF01016429}
}

@article{Tsallis1999,
    author = {C. Tsallis},
    title = {Nonextensive statistics: theoretical, experimental and computational evidences and connections},
    journal = {Braz. J. Phys.},
    volume = {29},
    year = {1999},
    pages = {1},
}

@article{Tsallis1998,
    author = {C. Tsallis and R.S. Mandes and A.R. Plastino},
    title = {The role of constraints within generalized nonextensive statistics},
    journal = {Physica A},
    volume = {261},
    year = {1998},
    pages = {534},
}

@misc{TsallisBibliography,
    author = {C. Tsallis},
    title = {Tsallis Bibliography},
    note = {\url{http://tsallis.cat.cbpf.br/biblio.htm}},
}

@article{RevieEstimators,
    author = {Hlavackova-Schindler, Katerina and Palus, Milan and Vejmelka, Martin and Bhattacharya, Joydeep},
    year = {2007},
    month = {02},
    pages = {},
    title = {Causality Detection Based on Information-Theoretic Approaches in Time Series Analysis},
    volume = {441 (2007) 1 – 46},
    journal = {Physics Reports}
}

% kernel Entropy Estimator

@book{silverman1986density,
    title = {Density Estimation for Statistics and Data Analysis},
    author = {Silverman, B.W.},
    year = {1986},
    publisher = {Chapman and Hall},
    address = {London},
    url = {http://dx.doi.org/10.1007/978-1-4899-3324-9}
}

@incollection{garcia-portuguesChapter2Kernel2025,
    title = {Chapter 2 {{Kernel}} Density Estimation},
    booktitle = {Notes for {{Nonparametric Statistics}}},
    author = {Garc\'ia-Portugu\'es, Eduardo},
    year = {2025},
    edition = {6.12.1},
    urldate = {2025-05-31},
    abstract = {Notes for Nonparametric Statistics. MSc in Statistics for Data Science. Carlos III University of Madrid.},
    isbn = {978-84-09-29537-1},
    url = {https://bookdown.org/egarpor/NP-UC3M/kde-i.html}
}

% KL Entropy estimator

@article{kozachenko1987sample,
    title = {Sample Estimate of the Entropy of a Random Vector},
    author = {Kozachenko, L.F. and Leonenko, N.N.},
    year = {1987},
    journal = {Problemy Peredachi Informatsii},
    volume = {23},
    pages = {95--100}
}

@article{laisantNumerationFactorielleApplication1888,
    title = {Sur La Num{\'e}ration Factorielle, Application Aux Permutations},
    author = {Laisant, C.- A.},
    year = {1888},
    journal = {Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
    volume = {2},
    pages = {176--183},
    issn = {0037-9484, 2102-622X},
    doi = {10.24033/bsmf.378},
    urldate = {2024-10-10}
}

@inproceedings{Lehmer1960TeachingCT,
    title = {Teaching Combinatorial Tricks to a Computer},
    booktitle = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
    author = {Lehmer, D. H.},
    year = {1960},
    series = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
    volume = {10},
    publisher = {American Mathematical Society},
    address = {Providence, Rhode Island},
    doi = {10.1090/psapm/010}
}

# Renyi Entropy

@article{leonenkoClassRenyiInformation2008,
    title = {A Class of {{R{\'e}nyi}} Information Estimators for Multidimensional Densities},
    author = {Leonenko, Nikolai and Pronzato, Luc and Savani, Vippal},
    year = {2008},
    journal = {The Annals of Statistics},
    volume = {36},
    number = {5},
    pages = {2153--2182},
    publisher = {Institute of Mathematical Statistics},
    doi = {10.1214/07-AOS539},
    keywords = {Entropy estimation,estimation of divergence,estimation of statistical distance,Havrda-Charvat entropy,nearest-neighbor distances,Renyi entropy,Tsallis entropy}
}

@inproceedings{leonenkoEstimationEntropiesDivergences2006,
    title = {Estimation of Entropies and Divergences via Nearest Neighbors},
    booktitle = {{{ProbaStat}} 2006},
    author = {Leonenko, Nikolai and Pronzato, Luc and Savani, Vippal},
    year = {2006},
    month = jun,
    volume = {39},
    pages = {265--273},
    address = {Smolenice, Slovakia},
    hal_id = {hal-00322783},
    hal_version = {v1},
    keywords = {entropy estimation,estimation of divergence,estimation of statistical distance,Havrda-Chav\`at entropy,nearest-neighbor distances,R\'enyi entropy,Tsallis entropy}
}

@article{miKSG2004,
    title = {Erratum: {{Estimating}} Mutual Information [Phys. {{Rev}}. {{E}} 69, 066138 (2004)]},
    author = {Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
    year = {2011},
    month = jan,
    journal = {Physical Review E},
    volume = {83},
    doi = {10.1103/PhysRevE.83.019903}
}

@misc{williamsGeneralizedMeasuresInformation2011,
  title = {Generalized {{Measures}} of {{Information Transfer}}},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2011},
  month = feb,
  number = {arXiv:1102.1507},
  eprint = {1102.1507},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1102.1507},
  urldate = {2025-04-16},
  abstract = {Transfer entropy provides a general tool for analyzing the magnitudes and directions---but not the {\textbackslash}emph\{kinds\}---of information transfer in a system. We extend transfer entropy in two complementary ways. First, we distinguish state-dependent from state-independent transfer, based on whether a source's influence depends on the state of the target. Second, for multiple sources, we distinguish between unique, redundant, and synergistic transfer. The new measures are demonstrated on several systems that extend examples from previous literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Dynamical Systems,Mathematics - Information Theory,Physics - Data Analysis Statistics and Probability}
}


% TE

@article{Schreiber.paper,
    title = {Measuring Information Transfer},
    author = {Schreiber, Thomas},
    journal = {Phys. Rev. Lett.},
    volume = {85},
    issue = {2},
    pages = {461--464},
    numpages = {0},
    year = {2000},
    month = {Jul},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.85.461},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.85.461}
}

@article{article_KSG_TE,
    author = {Gomez-Herrero, German and Wu, Wei and Rutanen, Kalle and Soriano, Miguel and Pipa, Gordon and Vicente, Raul},
    year = {2010},
    month = {08},
    pages = {},
    title = {Assessing Coupling Dynamics from an Ensemble of Time Series},
    volume = {17},
    journal = {Entropy},
    doi = {10.3390/e17041958}
}

@article{article_eTE_Computation,
    author = {Marschinski, R. and Kantz, H.},
    year = {2002},
    month = {11},
    pages = {275-281},
    title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
    volume = {30},
    journal = {European Physical Journal B},
    doi = {10.1140/epjb/e2002-00379-2}
}

@article{articleKantz,
    author = {Marschinski, R. and Kantz, H.},
    year = {2002},
    month = {11},
    pages = {275-281},
    title = {Analysing the information flow between financial time series . An improved estimator for transfer entropy},
    volume = {30},
    journal = {European Physical Journal B},
    doi = {10.1140/epjb/e2002-00379-2}
}

@article{TE_Kernel_Kaiser,
    author = {Kaiser, A. and Schreiber, Thomas},
    year = {2002},
    month = {06},
    pages = {},
    title = {Information transfer in continuous processes},
    volume = {166},
    journal = {Physica D, v.166, 43-62 (2002)},
    doi = {10.1016/S0167-2789(02)00432-3}
}

@article{KSG_TE,
    doi = {10.1371/journal.pone.0055809},
    author = {Wibral, Michael AND Pampu, Nicolae AND Priesemann, Viola AND Siebenhühner, Felix AND Seiwert, Hannes AND Lindner, Michael AND Lizier, Joseph T. AND Vicente, Raul},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Measuring Information-Transfer Delays},
    year = {2013},
    month = {02},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pone.0055809},
    pages = {1-19},
    abstract = {In complex networks such as gene networks, traffic systems or brain circuits it is important to understand how long it takes for the different parts of the network to effectively influence one another. In the brain, for example, axonal delays between brain areas can amount to several tens of milliseconds, adding an intrinsic component to any timing-based processing of information. Inferring neural interaction delays is thus needed to interpret the information transfer revealed by any analysis of directed interactions across brain structures. However, a robust estimation of interaction delays from neural activity faces several challenges if modeling assumptions on interaction mechanisms are wrong or cannot be made. Here, we propose a robust estimator for neuronal interaction delays rooted in an information-theoretic framework, which allows a model-free exploration of interactions. In particular, we extend transfer entropy to account for delayed source-target interactions, while crucially retaining the conditioning on the embedded target state at the immediately previous time step. We prove that this particular extension is indeed guaranteed to identify interaction delays between two coupled systems and is the only relevant option in keeping with Wiener’s principle of causality. We demonstrate the performance of our approach in detecting interaction delays on finite data by numerical simulations of stochastic and deterministic processes, as well as on local field potential recordings. We also show the ability of the extended transfer entropy to detect the presence of multiple delays, as well as feedback loops. While evaluated on neuroscience data, we expect the estimator to be useful in other fields dealing with network dynamics.},
    number = {2},

}

@article{Symbolic_TE,
    title = {Symbolic Transfer Entropy},
    author = {Staniek, Matth\"aus and Lehnertz, Klaus},
    journal = {Phys. Rev. Lett.},
    volume = {100},
    issue = {15},
    pages = {158101},
    numpages = {4},
    year = {2008},
    month = {Apr},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.100.158101},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.158101}
}

#JSD

@ARTICLE{Divergence_measures_Lin,
    author = {Lin, J.},
    journal = {IEEE Transactions on Information Theory},
    title = {Divergence measures based on the Shannon entropy},
    year = {1991},
    volume = {37},
    number = {1},
    pages = {145-151},
    keywords = {Entropy;Probability distribution;Upper bound;Pattern analysis;Signal analysis;Signal processing;Pattern recognition;Taxonomy;Genetics;Computer science},
    doi = {10.1109/18.61115} }

# Conditional MI (KSG)

@article{CMI_KSG_Frenzel_Pompe,
    title = {Partial Mutual Information for Coupling Analysis of Multivariate Time Series},
    author = {Frenzel, Stefan and Pompe, Bernd},
    journal = {Phys. Rev. Lett.},
    volume = {99},
    issue = {20},
    pages = {204101},
    numpages = {4},
    year = {2007},
    month = {Nov},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.99.204101},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.204101}
}

#local TE

@article{local_TE_Lizier,
    title = {Local information transfer as a spatiotemporal filter for complex systems},
    author = {Lizier, Joseph T. and Prokopenko, Mikhail and Zomaya, Albert Y.},
    journal = {Phys. Rev. E},
    volume = {77},
    issue = {2},
    pages = {026110},
    numpages = {11},
    year = {2008},
    month = {Feb},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevE.77.026110},
    url = {https://link.aps.org/doi/10.1103/PhysRevE.77.026110}
}

@article{lizierJIDTInformationTheoreticToolkit2014,
  title = {{{JIDT}}: {{An Information-Theoretic Toolkit}} for {{Studying}} the {{Dynamics}} of {{Complex Systems}}},
  shorttitle = {{{JIDT}}},
  author = {Lizier, Joseph T.},
  year = {2014},
  month = dec,
  journal = {Frontiers in Robotics and AI},
  volume = {1},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2014.00011},
  urldate = {2025-04-10},
  abstract = {Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics, and artificial life. This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyze the dynamics of complex systems in these fields. We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project, which provides a standalone (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data. While the toolkit provides classic information-theoretic measures (e.g., entropy, mutual information, and conditional mutual information), it ultimately focuses on implementing higher-level measures for information dynamics. That is, JIDT focuses on quantifying information storage, transfer, and modification, and the dynamics of these operations in space and time. For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants. JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g., Gaussian, box-kernel, and Kraskov--St{\"o}gbauer--Grassberger), which can be swapped at run-time due to Java's object-oriented polymorphism. Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave, Python, and other environments. We present the principles behind the code design, and provide several examples to guide users.},
  langid = {english},
  keywords = {complex networks,complex systems,information storage,Information Theory,information transfer,Java,MATLAB,Octave,python,transfer entropy}
}



#conditional TE article

@Article{cond_te_Ostargard,
    AUTHOR = {Shahsavari Baboukani, Payam and Graversen, Carina and Alickovic, Emina and Østergaard, Jan},
    TITLE = {Estimating Conditional Transfer Entropy in Time Series Using Mutual Information and Nonlinear Prediction},
    JOURNAL = {Entropy},
    VOLUME = {22},
    YEAR = {2020},
    NUMBER = {10},
    ARTICLE-NUMBER = {1124},
    URL = {https://www.mdpi.com/1099-4300/22/10/1124},
    PubMedID = {33286893},
    ISSN = {1099-4300},
    ABSTRACT = {We propose a new estimator to measure directed dependencies in time series. The dimensionality of data is first reduced using a new non-uniform embedding technique, where the variables are ranked according to a weighted sum of the amount of new information and improvement of the prediction accuracy provided by the variables. Then, using a greedy approach, the most informative subsets are selected in an iterative way. The algorithm terminates, when the highest ranked variable is not able to significantly improve the accuracy of the prediction as compared to that obtained using the existing selected subsets. In a simulation study, we compare our estimator to existing state-of-the-art methods at different data lengths and directed dependencies strengths. It is demonstrated that the proposed estimator has a significantly higher accuracy than that of existing methods, especially for the difficult case, where the data are highly correlated and coupled. Moreover, we show its false detection of directed dependencies due to instantaneous couplings effect is lower than that of existing measures. We also show applicability of the proposed estimator on real intracranial electroencephalography data.},
    DOI = {10.3390/e22101124}
}

@ARTICLE{JSD_distance_Endres,
    author = {Endres, D.M. and Schindelin, J.E.},
    journal = {IEEE Transactions on Information Theory},
    title = {A new metric for probability distributions},
    year = {2003},
    volume = {49},
    number = {7},
    pages = {1858-1860},
    keywords = {Gaussian noise;Probability distribution;Iterative algorithms;Writing;Algorithm design and analysis;Wavelet analysis;Adaptive estimation;White noise;Bayesian methods;Convergence},
    doi = {10.1109/TIT.2003.813506} }

@inproceedings{versteegInformationtheoreticMeasuresInfluence2013,
  title = {Information-Theoretic Measures of Influence Based on Content Dynamics},
  booktitle = {Proceedings of the Sixth {{ACM}} International Conference on {{Web}} Search and Data Mining},
  author = {Ver Steeg, Greg and Galstyan, Aram},
  year = {2013},
  month = feb,
  series = {{{WSDM}} '13},
  pages = {3--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2433396.2433400},
  urldate = {2025-04-16},
  abstract = {The fundamental building block of social influence is for one person to elicit a response in another. Researchers measuring a "response" in social media typically depend either on detailed models of human behavior or on platform-specific cues such as re-tweets, hash tags, URLs, or mentions. Most content on social networks is difficult to model because the modes and motivation of human expression are diverse and incompletely understood. We introduce content transfer, an information-theoretic measure with a predictive interpretation that directly quantifies the strength of the effect of one user's content on another's in a model-free way. Estimating this measure is made possible by combining recent advances in non-parametric entropy estimation with increasingly sophisticated tools for content representation. We demonstrate on Twitter data collected for thousands of users that content transfer is able to capture non-trivial, predictive relationships even for pairs of users not linked in the follower or mention graph. We suggest that this measure makes large quantities of previously under-utilized social media content accessible to rigorous statistical causal analysis.},
  isbn = {978-1-4503-1869-3}
}

@article{grassbergerFiniteSampleCorrections1988,
  title = {Finite Sample Corrections to Entropy and Dimension Estimates},
  author = {Grassberger, Peter},
  year = {1988},
  month = apr,
  journal = {Physics Letters A},
  volume = {128},
  number = {6},
  pages = {369--373},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(88)90193-4},
  urldate = {2025-06-24},
  abstract = {We derive the systematic corrections to estimates of generalized (Renyi) entropies and to generalized dimensions Dq from finite data sets. As an application, we discuss correlation estimates of Dq for the H{\'e}non map. We end with some remarks about lacunarity measures.}
}

@misc{grassbergerEntropyEstimatesInsufficient2008,
  title = {Entropy {{Estimates}} from {{Insufficient Samplings}}},
  author = {Grassberger, P.},
  year = {2008},
  month = jan,
  number = {arXiv:physics/0307138},
  eprint = {physics/0307138},
  publisher = {arXiv},
  doi = {10.48550/arXiv.physics/0307138},
  urldate = {2025-06-24},
  abstract = {We present a detailed derivation of some estimators of Shannon entropy for discrete distributions. They hold for finite samples of N points distributed into M "boxes", with N and M -{$>$} oo, but N/M {$<$} oo. In the high sampling regime ({$<<$} 1 points in each box) they have exponentially small biases. In the low sampling regime the errors increase but are still much smaller than for most other estimators. One advantage is that our main estimators are given analytically, with explicitly known analytical formulas for the biases.},
  archiveprefix = {arXiv},
  keywords = {Physics - Computational Physics,Physics - Data Analysis Statistics and Probability}
}

@article{hausserEntropyInferenceJamesStein2009,
  title = {Entropy {{Inference}} and the {{James-Stein Estimator}}, with {{Application}} to {{Nonlinear Gene Association Networks}}},
  author = {Hausser, Jean and Strimmer, Korbinian},
  year = {2009},
  month = dec,
  journal = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {1469--1484},
  issn = {1532-4435},
  abstract = {We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.}
}

@article{krichevskyPerformanceUniversalEncoding1981,
  title = {The Performance of Universal Encoding},
  author = {Krichevsky, R. and Trofimov, V.},
  year = {1981},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {27},
  number = {2},
  pages = {199--207},
  issn = {1557-9654},
  doi = {10.1109/TIT.1981.1056331},
  abstract = {Universal coding theory is surveyed from the viewpoint of the interplay between delay and redundancy. The price for universality turns out to be acceptably small.}
}

@article{bayesEssaySolvingProblem1763,
  title = {An {{Essay}} towards {{Solving}} a {{Problem}} in the {{Doctrine}} of {{Chances}}. {{By}} the {{Late Rev}}. {{Mr}}. {{Bayes}}, {{F}}. {{R}}. {{S}}. {{Communicated}} by {{Mr}}. {{Price}}, in a {{Letter}} to {{John Canton}}, {{A}}. {{M}}. {{F}}. {{R}}. {{S}}.},
  author = {Bayes, {Mr}. and Price, {Mr}.},
  year = {1763},
  journal = {Philosophical Transactions (1683-1775)},
  volume = {53},
  eprint = {105741},
  eprinttype = {jstor},
  pages = {370--418},
  publisher = {The Royal Society},
  issn = {0260-7085}
}

@article{chaoEntropySpeciesAccumulation2013,
  title = {Entropy and the Species Accumulation Curve: A Novel Entropy Estimator via Discovery Rates of New Species},
  shorttitle = {Entropy and the Species Accumulation Curve},
  author = {Chao, Anne and Wang, Y. T. and Jost, Lou},
  year = {2013},
  journal = {Methods in Ecology and Evolution},
  volume = {4},
  number = {11},
  pages = {1091--1100},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12108},
  abstract = {Estimating Shannon entropy and its exponential from incomplete samples is a central objective of many research fields. However, empirical estimates of Shannon entropy and its exponential depend strongly on sample size and typically exhibit substantial bias. This work uses a novel method to obtain an accurate, low-bias analytic estimator of entropy, based on species frequency counts. Our estimator does not require prior knowledge of the number of species. We show that there is a close relationship between Shannon entropy and the species accumulation curve, which depicts the cumulative number of observed species as a function of sample size. We reformulate entropy in terms of the expected discovery rates of new species with respect to sample size, that is, the successive slopes of the species accumulation curve. Our estimator is obtained by applying slope estimators derived from an improved Good-Turing frequency formula. Our method is also applied to estimate mutual information. Extensive simulations from theoretical models and real surveys show that if sample size is not unreasonably small, the resulting entropy estimator is nearly unbiased. Our estimator generally outperforms previous methods in terms of bias and accuracy (low mean squared error) especially when species richness is large and there is a large fraction of undetected species in samples. We discuss the extension of our approach to estimate Shannon entropy for multiple incidence data. The use of our estimator in constructing an integrated rarefaction and extrapolation curve of entropy (or mutual information) as a function of sample size or sample coverage (an aspect of sample completeness) is also discussed.},
  langid = {english},
  keywords = {diversity,Good-Turing frequency formula,mutual information,sample coverage,Shannon entropy,species accumulation curve,species discovery rate}
}

@article{nemenmanEntropyInformationNeural2004,
  title = {Entropy and Information in Neural Spike Trains: {{Progress}} on the Sampling Problem},
  shorttitle = {Entropy and Information in Neural Spike Trains},
  author = {Nemenman, Ilya and Bialek, William and van Steveninck, Rob de Ruyter},
  year = {2004},
  month = may,
  journal = {Physical Review E},
  volume = {69},
  number = {5},
  eprint = {physics/0306063},
  pages = {056111},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.056111},
  urldate = {2025-06-27},
  abstract = {The major problem in information theoretic analysis of neural responses and other biological data is the reliable estimation of entropy--like quantities from small samples. We apply a recently introduced Bayesian entropy estimator to synthetic data inspired by experiments, and to real experimental spike trains. The estimator performs admirably even very deep in the undersampled regime, where other techniques fail. This opens new possibilities for the information theoretic analysis of experiments, and may be of general interest as an example of learning from limited data.},
  archiveprefix = {arXiv},
  keywords = {Physics - Biological Physics,Physics - Data Analysis Statistics and Probability,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods}
}

@misc{nemenmanEntropyInferenceRevisited2002,
  title = {Entropy and Inference, Revisited},
  author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
  year = {2002},
  month = jan,
  number = {arXiv:physics/0108025},
  eprint = {physics/0108025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.physics/0108025},
  urldate = {2025-06-27},
  abstract = {We study properties of popular near-uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam-style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions.},
  archiveprefix = {arXiv},
  keywords = {Physics - Data Analysis Statistics and Probability}
}

@article{bonachelaEntropyEstimatesSmall2008,
  title = {Entropy Estimates of Small Data Sets},
  author = {Bonachela, Juan A. and Hinrichsen, Haye and Munoz, Miguel A.},
  year = {2008},
  month = may,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {41},
  number = {20},
  eprint = {0804.4561},
  primaryclass = {cond-mat},
  pages = {202001},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/41/20/202001},
  urldate = {2025-06-27},
  abstract = {Estimating entropies from limited data series is known to be a non-trivial task. Naive estimations are plagued with both systematic (bias) and statistical errors. Here, we present a new 'balanced estimator' for entropy functionals Shannon, R{\textbackslash}'enyi and Tsallis) specially devised to provide a compromise between low bias and small statistical errors, for short data series. This new estimator out-performs other currently available ones when the data sets are small and the probabilities of the possible outputs of the random variable are not close to zero. Otherwise, other well-known estimators remain a better choice. The potential range of applicability of this estimator is quite broad specially for biological and digital data series.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,Quantitative Biology - Quantitative Methods}
}

@article{grabchakAuthorshipAttributionUsing2013,
  title = {Authorship {{Attribution Using Entropy}}},
  author = {Grabchak, M. and Zhang, Z. and Zhang, D. T.},
  year = {2013},
  month = nov,
  journal = {Journal of Quantitative Linguistics},
  volume = {20},
  number = {4},
  pages = {301--313},
  publisher = {Routledge},
  issn = {0929-6174},
  doi = {10.1080/09296174.2013.830551},
  abstract = {We propose a new methodology for testing the authorship of a relatively small work compared with the large body of an author's cannon. Our approach is based on comparing the entropy of the two samples. The difficulty lies in the fact that known estimators of entropy tend to have a large bias even when the sample size is fairly large. To deal with this, we suggest splitting the larger sample into several parts of length equal to the length of the smaller work. We then propose using these new sub-samples in a simple non-parametric test. We apply our methodology to test whether the poem ``Shall I Die?'' which is sometimes attributed to William Shakespeare was, in fact, written by him.}
}


@misc{lozanoFastCalculationEntropy2017,
  title = {Fast Calculation of Entropy with {{Zhang}}'s Estimator},
  author={Antoni Lozano and Bernardino Casas and Chris Bentz and Ramon Ferrer-i-Cancho},
  year = {2017},
  month = jul,
  number = {arXiv:1707.08290},
  eprint = {1707.08290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.08290},
  urldate = {2025-06-27},
  abstract = {Entropy is a fundamental property of a repertoire. Here, we present an efficient algorithm to estimate the entropy of types with the help of Zhang's estimator. The algorithm takes advantage of the fact that the number of different frequencies in a text is in general much smaller than the number of types. We justify the convenience of the algorithm by means of an analysis of the statistical properties of texts from more than 1000 languages. Our work opens up various possibilities for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{chaoNonparametricEstimationShannons2003,
  title = {Nonparametric Estimation of {{Shannon}}'s Index of Diversity When There Are Unseen Species in Sample},
  author = {Chao, Anne and Shen, Tsung-Jen},
  year = {2003},
  month = dec,
  journal = {Environmental and Ecological Statistics},
  volume = {10},
  number = {4},
  pages = {429--443},
  issn = {1573-3009},
  doi = {10.1023/A:1026096204727},
  urldate = {2025-06-28},
  abstract = {A biological community usually has a large number of species with relatively small abundances. When a random sample of individuals is selected and each individual is classified according to species identity, some rare species may not be discovered. This paper is concerned with the estimation of Shannon's index of diversity when the number of species and the species abundances are unknown. The traditional estimator that ignores the missing species underestimates when there is a non-negligible number of unseen species. We provide a different approach based on unequal probability sampling theory because species have different probabilities of being discovered in the sample. No parametric forms are assumed for the species abundances. The proposed estimation procedure combines the Horvitz--Thompson (1952) adjustment for missing species and the concept of sample coverage, which is used to properly estimate the relative abundances of species discovered in the sample. Simulation results show that the proposed estimator works well under various abundance models even when a relatively large fraction of the species is missing. Three real data sets, two from biology and the other one from numismatics, are given for illustration.},
  langid = {english},
  keywords = {biodiversity,Distribution Theory,entropy,Horvitz-Thompson estimator,jackknife,Non-parametric Inference,Parametric Inference,sample coverage,species,Statistical Theory and Methods,Statistics,Theoretical and Statistical Ecology,unequal probability sampling}
}

@article{kellyDiscreteEntropyjlEntropyEstimation2024,
  title = {{{DiscreteEntropy}}.Jl: {{Entropy Estimation}} of {{Discrete Random Variables}} with {{Julia}}},
  shorttitle = {{{DiscreteEntropy}}.Jl},
  author = {Kelly, David A. and Torre, Ilaria Pia La},
  year = {2024},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {9},
  number = {103},
  pages = {7334},
  issn = {2475-9066},
  doi = {10.21105/joss.07334},
  urldate = {2025-06-27},
  abstract = {Kelly et al., (2024). DiscreteEntropy.jl: Entropy Estimation of Discrete Random Variables with Julia. Journal of Open Source Software, 9(103), 7334, https://doi.org/10.21105/joss.07334},
  langid = {english}
}

@article{degregorioEntropyEstimatorsMarkovian2024,
  title = {Entropy {{Estimators}} for {{Markovian Sequences}}: {{A Comparative Analysis}}},
  shorttitle = {Entropy {{Estimators}} for {{Markovian Sequences}}},
  author = {De Gregorio, Juan and S{\'a}nchez, David and Toral, Ra{\'u}l},
  year = {2024},
  month = jan,
  journal = {Entropy},
  volume = {26},
  number = {1},
  pages = {79},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e26010079},
  urldate = {2025-06-28},
  abstract = {Entropy estimation is a fundamental problem in information theory that has applications in various fields, including physics, biology, and computer science. Estimating the entropy of discrete sequences can be challenging due to limited data and the lack of unbiased estimators. Most existing entropy estimators are designed for sequences of independent events and their performances vary depending on the system being studied and the available data size. In this work, we compare different entropy estimators and their performance when applied to Markovian sequences. Specifically, we analyze both binary Markovian sequences and Markovian systems in the undersampled regime. We calculate the bias, standard deviation, and mean squared error for some of the most widely employed estimators. We discuss the limitations of entropy estimation as a function of the transition probabilities of the Markov processes and the sample size. Overall, this paper provides a comprehensive comparison of entropy estimators and their performance in estimating entropy for systems with memory, which can be useful for researchers and practitioners in various fields.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {data analysis,estimators,Markovian systems,Shannon entropy}
}

@article{marconEntropartPackageMeasure2015,
  title = {{{entropart}}: {{An R}} Package to Measure and Partition Diversity},
  author = {Marcon, Eric and H{\'e}rault, Bruno},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {8},
  pages = {1--26},
  doi = {10.18637/jss.v067.i08}
}
